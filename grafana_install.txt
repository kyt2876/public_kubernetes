https://www.gojek.io/blog/diy-set-up-telegraf-influxdb-grafana-on-kubernetes
Setup InfluxDB
db비밀번호 secret 저장
영구볼륨
Deployment
Expose this deployment using a service

monitoring 네임스페이스 저장
1 kubectl create namespace monitoring
vi grafna_namespace.yaml
---
apiVersion: v1
kind: Namespace
metadata:
    name: monitoring
---

kubectl create -f grafna_namespace.yaml

kubectl config set-context monitoring-admin@kubernetes --cluster=kubernetes --user=kubernetes-admin --namespace=monitoring
kubectl config use-context monitoring-admin@kubernetes

2. db비밀번호 secret 저장
kubectl create secret generic influxdb-creds \
  --from-literal=INFLUXDB_DATABASE=local_monitoring \
  --from-literal=INFLUXDB_USERNAME=root \
  --from-literal=INFLUXDB_PASSWORD=root1234 \
  --from-literal=INFLUXDB_HOST=influxdb \
--dry-run -o yaml > grafana_secret.yaml

kubectl create -f grafana_secret.yaml -n monitoring
kubectl get secrets -n monitoring


3. Persistent volume for InfluxDB
vi pvc_influxdb.yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  labels:
    app: influxdb
  name: influxdb-pvc
spec:
  storageClassName: "nfs-client"
  accessModes: ["ReadWriteOnce"]
  resources:
    requests:
      storage: 1Gi
--- 

kubectl create -f pvc_influxdb.yaml -n monitoring
kubectl get pvc



InfluxDB Deployment

vi deployment_influxdb.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: monitoring
  annotations:
  creationTimestamp: null
  generation: 1
  labels:
    app: influxdb
  name: influxdb
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: influxdb
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: influxdb
    spec:
      containers:
      - envFrom:
        - secretRef:
            name: influxdb-creds
        image: docker.io/influxdb:1.6.4
        imagePullPolicy: IfNotPresent
        name: influxdb
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /var/lib/influxdb
          name: var-lib-influxdb
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
      volumes:
      - name: var-lib-influxdb
        persistentVolumeClaim:
          claimName: influxdb-pvc
----

kubectl create -f deployment_influxdb.yaml
kubectl get pods 



InfluxDB Service #kubectl expose deployment influxdb --port=8086 --target-port=8086 --protocol=TCP --type=nodeport-service

vi service_influxdb.yaml
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: influxdb
  name: influxdb
spec:
  type: NodePort
#  clusterIP:
  selector:
    app: influxdb
  ports:
  - protocol: TCP
    port: 8086
    targetPort: 8086
#    nodePort: 30211
---
kubectl create -f  service_influxdb.yaml
kubectl get svc -o wide

Telegraf Secret

vi secret_telgra.yaml
apiVersion: v1
kind: Secret
metadata:
  name: telegraf-secrets
type: Opaque
stringData:
  INFLUXDB_DB: local_monitoring
  INFLUXDB_URL: http://influxdb:8086
  INFLUXDB_USER: root
  INFLUXDB_USER_PASSWORD: root1234

kubectl create -f secret_telgra.yaml -n monitoring
kubectl get secrets

The application can start publishing StatsD events to (192.168.99.100:32161 from outside the cluster), or Telegraf’s service IP using UDP protocol.

Telegraf Config
vi telgraf_config.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: telegraf-config
data:
  telegraf.conf: |+
    [[outputs.influxdb]]
      urls = ["$INFLUXDB_URL"]
      database = "$INFLUXDB_DB"
      username = "$INFLUXDB_USER"
      password = "$INFLUXDB_USER_PASSWORD"
# Statsd Server
    [[inputs.statsd]]
      max_tcp_connections = 250
      tcp_keep_alive = false
      service_address = ":8125"
      delete_gauges = true
      delete_counters = true
      delete_sets = true
      delete_timings = true
      metric_separator = "."
      allowed_pending_messages = 10000
      percentile_limit = 1000
      parse_data_dog_tags = true 
      read_buffer_size = 65535
---
kubectl create -f telgraf_config.yaml
kubectl get configmaps

Telegraf Deployment
vi telgraf_deployment.yaml
----
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: monitoring
  name: telegraf
spec:
  selector:
    matchLabels:
      app: telegraf
  minReadySeconds: 5
  template:
    metadata:
      labels:
        app: telegraf
    spec:
      containers:
#       - image: telegraf:1.10.0
        - image: telegraf:latest
          name: telegraf
          envFrom:
            - secretRef:
                name: telegraf-secrets
          volumeMounts:
            - name: telegraf-config-volume
              mountPath: /etc/telegraf/telegraf.conf
              subPath: telegraf.conf
              readOnly: true
      volumes:
        - name: telegraf-config-volume
          configMap:
            name: telegraf-config

----
kubectl create -f telgraf_deployment.yaml
kubectl get pods

Telegraf Service
#kubectl expose deployment telegraf --port=8125 --target-port=8125 --protocol=UDP --type=NodePort
vi telegraf_service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: telegraf
spec:
  ports:
  - port: 8125
    protocol: UDP
    targetPort: 8125
  selector:
    app: telegraf
  type: NodePort
---
kubectl create -f telegraf_service.yaml

grafana-creds.yaml

kubectl create secret generic grafana-creds \
  --from-literal=GF_SECURITY_ADMIN_USER=admin \
  --from-literal=GF_SECURITY_ADMIN_PASSWORD=admin1234 \
--dry-run -o yaml > grafana_secret.yaml
kubectl create -f grafana_secret.yaml
kubectl get secrets

Grafana Deployment
vi grafana_deployment.yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: monitoring
  annotations:
  creationTimestamp: null
  generation: 1
  labels:
    app: grafana
  name: grafana
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: grafana
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: grafana
    spec:
      containers:
      - envFrom:
        - secretRef:
            name: grafana-creds
        image: docker.io/grafana/grafana:5.3.2
        imagePullPolicy: IfNotPresent
        name: grafana
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
---

kubectl create -f grafana_deployment.yaml
kubectl get pods

#kubectl expose deployment grafana --type=NodePort --port=3000 --target-port=3000 --protocol=TCP --dry-run -o yaml > grafana_service.yaml

kubectl create -f  grafana_service.yaml
kubectl get svc

kubectl get svc
NAME       TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
grafana    NodePort   10.108.168.119   <none>        3000:30304/TCP   20s
influxdb   NodePort   10.97.108.89     <none>        8086:31937/TCP   39m
telegraf   NodePort   10.96.86.216     <none>        8125:31290/UDP   19m

http://192.168.0.162:30304  admin/admin1234

데이터소스 추가 : InfluxDB
http://192.168.0.162:31937 //DB접속주소
DB장버   INFLUXDB_DB: local_monitoring
INFLUXDB_USER: root
INFLUXDB_USER_PASSWORD: root1234




도커 대시보드 10585